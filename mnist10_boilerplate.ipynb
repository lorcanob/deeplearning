{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "import torch\n",
    "\n",
    "fastbook.setup_book()\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *\n",
    "from fastbook import *\n",
    "path = Path('/storage/data/mnist_png')\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('training'),Path('testing')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ingest data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "num_dict = L(enumerate(num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#10) [(0, 'zero'),(1, 'one'),(2, 'two'),(3, 'three'),(4, 'four'),(5, 'five'),(6, 'six'),(7, 'seven'),(8, 'eight'),(9, 'nine')],\n",
       " 'four')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test cell\n",
    "num_dict, num_dict[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackNums(train=True):\n",
    "    num_cats = [num_dict[num][1] for num in range(0,10)]\n",
    "    if train==True:\n",
    "        num_path = [(path/'training'/str(num[0])).ls().sorted() for num in num_dict]\n",
    "    else:\n",
    "        num_path = [(path/'testing'/str(num[0])).ls().sorted() for num in num_dict]\n",
    "    return num_cats, num_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat,num_path = stackNums()\n",
    "valid_num_cat,valid_num_path = stackNums(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['zero',\n",
       "  'one',\n",
       "  'two',\n",
       "  'three',\n",
       "  'four',\n",
       "  'five',\n",
       "  'six',\n",
       "  'seven',\n",
       "  'eight',\n",
       "  'nine'],\n",
       " [(#5923) [Path('training/0/1.png'),Path('training/0/1000.png'),Path('training/0/10005.png'),Path('training/0/10010.png'),Path('training/0/10022.png'),Path('training/0/10025.png'),Path('training/0/10026.png'),Path('training/0/10045.png'),Path('training/0/10069.png'),Path('training/0/10071.png')...],\n",
       "  (#6742) [Path('training/1/10006.png'),Path('training/1/10007.png'),Path('training/1/1002.png'),Path('training/1/10020.png'),Path('training/1/10027.png'),Path('training/1/1003.png'),Path('training/1/10040.png'),Path('training/1/10048.png'),Path('training/1/10058.png'),Path('training/1/10067.png')...],\n",
       "  (#5958) [Path('training/2/10009.png'),Path('training/2/10016.png'),Path('training/2/10024.png'),Path('training/2/10029.png'),Path('training/2/10072.png'),Path('training/2/10073.png'),Path('training/2/10075.png'),Path('training/2/10078.png'),Path('training/2/10081.png'),Path('training/2/10082.png')...],\n",
       "  (#6131) [Path('training/3/10.png'),Path('training/3/10000.png'),Path('training/3/10011.png'),Path('training/3/10031.png'),Path('training/3/10034.png'),Path('training/3/10042.png'),Path('training/3/10052.png'),Path('training/3/1007.png'),Path('training/3/10074.png'),Path('training/3/10091.png')...],\n",
       "  (#5842) [Path('training/4/10013.png'),Path('training/4/10018.png'),Path('training/4/10033.png'),Path('training/4/1004.png'),Path('training/4/1006.png'),Path('training/4/10060.png'),Path('training/4/1008.png'),Path('training/4/10103.png'),Path('training/4/10104.png'),Path('training/4/10114.png')...],\n",
       "  (#5421) [Path('training/5/0.png'),Path('training/5/100.png'),Path('training/5/10008.png'),Path('training/5/10015.png'),Path('training/5/10030.png'),Path('training/5/10035.png'),Path('training/5/10049.png'),Path('training/5/10051.png'),Path('training/5/10056.png'),Path('training/5/10062.png')...],\n",
       "  (#5918) [Path('training/6/10017.png'),Path('training/6/10032.png'),Path('training/6/10036.png'),Path('training/6/10037.png'),Path('training/6/10044.png'),Path('training/6/10053.png'),Path('training/6/10076.png'),Path('training/6/10089.png'),Path('training/6/10101.png'),Path('training/6/10108.png')...],\n",
       "  (#6265) [Path('training/7/10002.png'),Path('training/7/1001.png'),Path('training/7/10014.png'),Path('training/7/10019.png'),Path('training/7/10039.png'),Path('training/7/10046.png'),Path('training/7/10050.png'),Path('training/7/10063.png'),Path('training/7/10077.png'),Path('training/7/10086.png')...],\n",
       "  (#5851) [Path('training/8/10001.png'),Path('training/8/10012.png'),Path('training/8/10021.png'),Path('training/8/10041.png'),Path('training/8/10054.png'),Path('training/8/10057.png'),Path('training/8/10061.png'),Path('training/8/10064.png'),Path('training/8/10066.png'),Path('training/8/10079.png')...],\n",
       "  (#5949) [Path('training/9/10003.png'),Path('training/9/10004.png'),Path('training/9/10023.png'),Path('training/9/10028.png'),Path('training/9/10038.png'),Path('training/9/10043.png'),Path('training/9/10047.png'),Path('training/9/1005.png'),Path('training/9/10055.png'),Path('training/9/10059.png')...]],\n",
       " [(#980) [Path('testing/0/10.png'),Path('testing/0/1001.png'),Path('testing/0/1009.png'),Path('testing/0/101.png'),Path('testing/0/1034.png'),Path('testing/0/1047.png'),Path('testing/0/1061.png'),Path('testing/0/1084.png'),Path('testing/0/1094.png'),Path('testing/0/1121.png')...],\n",
       "  (#1135) [Path('testing/1/1004.png'),Path('testing/1/1008.png'),Path('testing/1/1011.png'),Path('testing/1/1019.png'),Path('testing/1/1025.png'),Path('testing/1/1027.png'),Path('testing/1/1030.png'),Path('testing/1/1037.png'),Path('testing/1/1038.png'),Path('testing/1/1040.png')...],\n",
       "  (#1032) [Path('testing/2/1.png'),Path('testing/2/1002.png'),Path('testing/2/1016.png'),Path('testing/2/1031.png'),Path('testing/2/1036.png'),Path('testing/2/1049.png'),Path('testing/2/1050.png'),Path('testing/2/1053.png'),Path('testing/2/1056.png'),Path('testing/2/106.png')...],\n",
       "  (#1010) [Path('testing/3/1020.png'),Path('testing/3/1028.png'),Path('testing/3/1042.png'),Path('testing/3/1062.png'),Path('testing/3/1066.png'),Path('testing/3/1067.png'),Path('testing/3/1069.png'),Path('testing/3/1072.png'),Path('testing/3/1092.png'),Path('testing/3/1095.png')...],\n",
       "  (#982) [Path('testing/4/1010.png'),Path('testing/4/1015.png'),Path('testing/4/1023.png'),Path('testing/4/1024.png'),Path('testing/4/103.png'),Path('testing/4/1043.png'),Path('testing/4/1051.png'),Path('testing/4/1057.png'),Path('testing/4/1059.png'),Path('testing/4/1060.png')...],\n",
       "  (#892) [Path('testing/5/1003.png'),Path('testing/5/102.png'),Path('testing/5/1022.png'),Path('testing/5/1032.png'),Path('testing/5/1041.png'),Path('testing/5/1046.png'),Path('testing/5/1070.png'),Path('testing/5/1073.png'),Path('testing/5/1082.png'),Path('testing/5/1087.png')...],\n",
       "  (#958) [Path('testing/6/100.png'),Path('testing/6/1014.png'),Path('testing/6/1017.png'),Path('testing/6/1035.png'),Path('testing/6/1044.png'),Path('testing/6/1079.png'),Path('testing/6/1085.png'),Path('testing/6/1099.png'),Path('testing/6/11.png'),Path('testing/6/1106.png')...],\n",
       "  (#1028) [Path('testing/7/0.png'),Path('testing/7/1006.png'),Path('testing/7/1012.png'),Path('testing/7/1021.png'),Path('testing/7/1039.png'),Path('testing/7/1055.png'),Path('testing/7/1071.png'),Path('testing/7/1091.png'),Path('testing/7/1096.png'),Path('testing/7/1100.png')...],\n",
       "  (#974) [Path('testing/8/1007.png'),Path('testing/8/1018.png'),Path('testing/8/1026.png'),Path('testing/8/1029.png'),Path('testing/8/1033.png'),Path('testing/8/1052.png'),Path('testing/8/1068.png'),Path('testing/8/1074.png'),Path('testing/8/1093.png'),Path('testing/8/110.png')...],\n",
       "  (#1009) [Path('testing/9/1000.png'),Path('testing/9/1005.png'),Path('testing/9/1013.png'),Path('testing/9/104.png'),Path('testing/9/1045.png'),Path('testing/9/1048.png'),Path('testing/9/105.png'),Path('testing/9/1058.png'),Path('testing/9/1063.png'),Path('testing/9/108.png')...]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cell\n",
    "num_cat,num_path,valid_num_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING LONG\n",
    "train_0s = torch.stack([tensor(Image.open(o)) for o in num_path[0]]).float()/255\n",
    "train_1s = torch.stack([tensor(Image.open(o)) for o in num_path[1]]).float()/255\n",
    "train_2s = torch.stack([tensor(Image.open(o)) for o in num_path[2]]).float()/255\n",
    "train_3s = torch.stack([tensor(Image.open(o)) for o in num_path[3]]).float()/255\n",
    "train_4s = torch.stack([tensor(Image.open(o)) for o in num_path[4]]).float()/255\n",
    "train_5s = torch.stack([tensor(Image.open(o)) for o in num_path[5]]).float()/255\n",
    "train_6s = torch.stack([tensor(Image.open(o)) for o in num_path[6]]).float()/255\n",
    "train_7s = torch.stack([tensor(Image.open(o)) for o in num_path[7]]).float()/255\n",
    "train_8s = torch.stack([tensor(Image.open(o)) for o in num_path[8]]).float()/255\n",
    "train_9s = torch.stack([tensor(Image.open(o)) for o in num_path[9]]).float()/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Later consider using Pandas DataFrames as a means of sorting and assigning labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING LONG\n",
    "valid_0s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[0]]).float()/255\n",
    "valid_1s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[1]]).float()/255\n",
    "valid_2s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[2]]).float()/255\n",
    "valid_3s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[3]]).float()/255\n",
    "valid_4s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[4]]).float()/255\n",
    "valid_5s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[5]]).float()/255\n",
    "valid_6s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[6]]).float()/255\n",
    "valid_7s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[7]]).float()/255\n",
    "valid_8s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[8]]).float()/255\n",
    "valid_9s = torch.stack([tensor(Image.open(o)) for o in valid_num_path[9]]).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5923, 28, 28]), torch.Size([1010, 28, 28]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cell\n",
    "train_0s.shape, valid_3s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]),\n",
       " torch.Size([60000, 1]),\n",
       " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.cat([train_0s, train_1s, train_2s, train_3s, train_4s, train_5s, train_6s, train_7s, train_8s, train_9s]).view(-1, 28*28)\n",
    "train_y = tensor([0]*len(train_0s) + [1]*len(train_1s) + [2]*len(train_2s) + [3]*len(train_3s) + [4]*len(train_4s) + [5]*len(train_5s) + [6]*len(train_6s) + [7]*len(train_7s) + [8]*len(train_8s) + [9]*len(train_9s)).unsqueeze(1)\n",
    "dset = list(zip(train_x,train_y))\n",
    "\n",
    "#test\n",
    "train_x.shape,train_y.shape,train_y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5923,\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test cell\n",
    "len(train_0s), train_y[5920:5925]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([6]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test cell\n",
    "x,y = dset[torch.randint(0, train_x.shape[0], (1,))]\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'daikon'),\n",
       " ('banana', 'beetroot'),\n",
       " ('cherry', 'carrot'),\n",
       " ('elderberry', 'endive'),\n",
       " ('apple', 'artichoke')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test cell\n",
    "fruitslist = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n",
    "veglist = ['artichoke', 'beetroot', 'carrot', 'daikon', 'endive']\n",
    "biglist = list(zip(fruitslist,veglist))\n",
    "random.shuffle(biglist)\n",
    "biglist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([5]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x = torch.cat([valid_0s, valid_1s, valid_2s, valid_3s, valid_4s, valid_5s, valid_6s, valid_7s, valid_8s, valid_9s]).view(-1, 28*28)\n",
    "valid_y = tensor([0]*len(valid_0s) + [1]*len(valid_1s) + [2]*len(valid_2s) + [3]*len(valid_3s) + [4]*len(valid_4s) + [5]*len(valid_5s) + [6]*len(valid_6s) + [7]*len(valid_7s) + [8]*len(valid_8s) + [9]*len(valid_9s)).unsqueeze(1)\n",
    "valid_dset = list(zip(valid_x,valid_y))\n",
    "\n",
    "random.shuffle(dset)\n",
    "random.shuffle(valid_dset)\n",
    "\n",
    "#test\n",
    "x,y = dset[torch.randint(0, valid_x.shape[0], (1,))]\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(dset, batch_size=256, shuffle=True)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)\n",
    "\n",
    "#test\n",
    "xbOne,ybOne = first(dl)\n",
    "xbOne.shape,ybOne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[7],\n",
      "        [7],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [7],\n",
      "        [3],\n",
      "        [7],\n",
      "        [8],\n",
      "        [7]])\n"
     ]
    }
   ],
   "source": [
    "# test cell: Check training data is shuffled\n",
    "print(xbOne[:10])\n",
    "print(ybOne[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define architecture, parameters, loss, metrics</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initParam(size, std=0.1): #Values over ~0.16 cause validation accs to get stuck at 0.0\n",
    "    return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "def mnistModel(inputs): #Remember to adjust according to parameter depth\n",
    "    acts = inputs@w1 + b1\n",
    "    acts = acts.max(tensor(0.0))\n",
    "    acts = acts@w2 + b2\n",
    "    acts = acts.max(tensor(0.0))\n",
    "    acts = acts@w3 + b3\n",
    "    acts = acts.max(tensor(0.0))\n",
    "    acts = acts@w4 + b4\n",
    "    return acts\n",
    "\n",
    "def mnistLoss(acts, targs): #Softmax to CELoss eventually\n",
    "    preds = -torch.log_softmax(acts, dim=1)\n",
    "    idx = range(preds.size()[0])\n",
    "    loss = preds[idx,targs].mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(inputs, labels, model):\n",
    "    acts = model(inputs)\n",
    "    loss = mnistLoss(acts, labels[:,0])\n",
    "    loss.backward()\n",
    "    \n",
    "def train_epoch(model):\n",
    "    for inputs,labels in dl:\n",
    "        calc_grad(inputs, labels, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "def batch_accuracy(acts, labels): #Are these nested indices a bad idea, consider one hot encoding/embedding?\n",
    "    preds = -torch.log_softmax(acts, dim=1) #Not summing to 1 across relevant axis\n",
    "    pred_idx = range(preds.size()[0])\n",
    "    correct = [preds[entry].min() == preds[entry,labels[entry]] for entry in pred_idx] #Because -log, should I min() instead of max()?\n",
    "    return torch.Tensor(correct).float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(inputs), labels) for inputs,labels in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n",
    "\n",
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise parameters and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 layer model: [784, 500, 200, 80, 10]\n",
    "w1 = initParam((28*28,500))\n",
    "b1 = initParam(500)\n",
    "w2 = initParam((500,200))\n",
    "b2 = initParam(200)\n",
    "w3 = initParam((200,80))\n",
    "b3 = initParam(80)\n",
    "w4 = initParam((80,10))\n",
    "b4 = initParam(10)\n",
    "params = list([w1,b1,w2,b2,w3,b3,w4,b4])\n",
    "\n",
    "lr = 1e-1\n",
    "\n",
    "opt = BasicOptim(params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9008 0.9354 0.9538 0.9591 0.9527 0.9412 "
     ]
    }
   ],
   "source": [
    "learner = train_model(mnistModel, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test cell\n",
    "acts1 = xbOne@w1 + b1\n",
    "acts2 = acts1.max(tensor(0.0))\n",
    "acts3 = acts2@w2 + b2\n",
    "acts4 = acts3.max(tensor(0.0))\n",
    "acts5 = acts4@w3 + b3\n",
    "acts6 = acts5.max(tensor(0.0))\n",
    "acts7 = acts6@w4 + b4\n",
    "\n",
    "sm_preds = -torch.log_softmax(acts7, dim=-1)\n",
    "pred_idx = range(256)\n",
    "\n",
    "\n",
    "loss = sm_preds[pred_idx,ybOne].mean()\n",
    "\n",
    "#Regularise the activations to stop over/underflow\n",
    "#Regularisation/Weight decay can be found in 08_collab\n",
    "\n",
    "#xbOne.size()\n",
    "acts7.size()\n",
    "#torch.softmax(acts3, dim=0)[29]\n",
    "#torch.log(torch.softmax(acts3, dim=0)).size()[0]\n",
    "#loss\n",
    "\n",
    "#torch.softmax(acts7, dim=1)[5].sum()\n",
    "#sm_preds[5].sum()\n",
    "#correct = torch.Tensor([sm_preds[entry].max() == sm_preds[entry,ybOne[entry]] for entry in pred_idx])\n",
    "#correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axis = tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).float()\n",
    "torch.softmax(axis,dim=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So:<br>\n",
    "I am summing the softmaxes properly; they do sum to 1<br>\n",
    "I THINK I am updating the parameters properly using grad times lr<br>\n",
    "But for some reason the weights aren't actually updating the way I would expect, because the loss is hovering around -log(0.1)=2.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
